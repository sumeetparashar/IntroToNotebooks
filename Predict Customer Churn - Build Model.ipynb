{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 2 with Spark 2.0", "name": "python2-spark20"}, "language_info": {"version": "2.7.11", "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Predict Customer Churn Use Case Implementation\nThe objective is to follow the CRISP-DM methodology to build a model to predict customer churn\n![CRISP-DM](https://raw.githubusercontent.com/yfphoon/dsx_demo/master/crisp_dm.png)\n\n### Step 1: Download the customer churn data"}, {"execution_count": null, "cell_type": "code", "source": "#Run once to install the wget package\n!pip install wget", "metadata": {}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# download data from GitHub repository\nimport wget\nurl_churn='https://raw.githubusercontent.com/yfphoon/dsx_demo/master/data/customer_churn/churn.csv'\nurl_customer='https://raw.githubusercontent.com/yfphoon/dsx_demo/master/data/customer_churn/customer.csv'\n\n#remove existing files before downloading\n!rm -f churn.csv\n!rm -f customer.csv\n\nchurnFilename=wget.download(url_churn)\ncustomerFilename=wget.download(url_customer)\n\n#list existing files\n!ls -l churn.csv\n!ls -l customer.csv", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 2: Read data into Spark DataFrames\n\nNote: You want to reference the Spark DataFrame API to learn more about the supported operations, https://spark.apache.org/docs/2.0.0-preview/api/python/pyspark.sql.html#pyspark.sql.DataFrame"}, {"execution_count": null, "cell_type": "code", "source": "churn= sqlContext.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(churnFilename)\ncustomer= sqlContext.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(customerFilename)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 3: Merge Files\n"}, {"execution_count": null, "cell_type": "code", "source": "data=customer.join(churn,customer['ID']==churn['ID']).select(customer['*'],churn['CHURN'])\ndata.toPandas().head()", "metadata": {"scrolled": false}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 4: Rename some columns\nThis step is not a requirement, it just makes some columns names simpler to type with no spaces"}, {"execution_count": null, "cell_type": "code", "source": "# withColumnRenamed renames an existing column in a SparkDataFrame and returns a new SparkDataFrame\n\ndata = data.withColumnRenamed(\"Est Income\", \"EstIncome\").withColumnRenamed(\"Car Owner\",\"CarOwner\")\ndata.toPandas().head()", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 5: Data understanding"}, {"metadata": {}, "cell_type": "markdown", "source": "### Dataset Overview"}, {"execution_count": null, "cell_type": "code", "source": "df_pandas = data.toPandas()\nprint \"There are \" + str(len(df_pandas)) + \" observations in the customer history dataset.\"\nprint \"There are \" + str(len(df_pandas.columns)) + \" variables in the dataset.\"\n\nprint \"\\n******************Descriptive statistics*****************************\\n\"\nprint df_pandas.drop(['ID'], axis = 1).describe()\n", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Exploratory Data Analysis"}, {"metadata": {}, "cell_type": "markdown", "source": "The **Brunel** Visualization Language is a highly succinct and novel language that defines interactive data visualizations based on tabular data. The language is well suited for both data scientists and more aggressive business users. The system interprets the language and produces visualizations using the user's choice of existing lower-level visualization technologies typically used by application engineers such as RAVE or D3. \n\nMore information about Brunel Visualization: https://github.com/Brunel-Visualization/Brunel/wiki\n\nTry Brunel visualization here:  http://brunel.mybluemix.net/gallery_app/renderer"}, {"execution_count": null, "cell_type": "code", "source": "import brunel\ndf_pandas = data.toPandas()\n%brunel data('df_pandas') stack bar x(Paymethod) y(#count) color(CHURN) bin(Paymethod) percent(#count) label(#count) tooltip(#all) | x(LongDistance) y(Usage) point color(Paymethod) tooltip(LongDistance, Usage) :: width=1100, height=400 ", "metadata": {}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# Heat map\n%brunel data('df_pandas') x(LocalBilltype) y(Dropped) color(#count:red) style('symbol:rect; size:100%; stroke:none') tooltip(Dropped,#count)", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**PixieDust** is a Python Helper library for Spark IPython Notebooks. One of it's main features are visualizations. You'll notice that unlike other APIs which produce just output, PixieDust creates an interactive UI in which you can explore data.<br/>\nMore information about PixieDust: https://github.com/ibm-cds-labs/pixiedust?cm_mc_uid=78151411419314871783930&cm_mc_sid_50200000=1487962969"}, {"metadata": {}, "cell_type": "markdown", "source": "**If you haven't already installed it, uncomment and run the following cell to install the pixiedust Python library in your notebook environment. You only need to run it once**\n"}, {"execution_count": null, "cell_type": "code", "source": "#!pip install --user --upgrade pixiedust", "metadata": {"scrolled": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "from pixiedust.display import *\ndisplay(data)", "metadata": {"pixiedust": {"displayParams": {"keyFields": "Paymethod", "rowCount": "500", "handlerId": "barChart", "aggregation": "AVG", "chartsize": "50", "valueFields": "Usage"}}}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Interactive query with Spark SQL"}, {"execution_count": null, "cell_type": "code", "source": "# Spark SQL also allow you to use standard SQL\ndata.createOrReplaceTempView(\"data\")\nsql = \"\"\"\nSELECT c.*\nFROM data c\nWHERE c.EstIncome>90000\n\n\"\"\"\nspark.sql(sql).toPandas().head()", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 6: Build the Spark pipeline and the Random Forest model\n\"Pipeline\" is an API in SparkML that's used for building models. A pipeline defines a sequence of transformers and estimators to perform tha analysis in stages.<br/>\nAdditional information on SparkML: https://spark.apache.org/docs/2.0.2/ml-guide.html"}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import RandomForestClassifier\n\n# StringIndexer encodes a string column of labels to a column of label indices. \nSI1 = StringIndexer(inputCol='Gender', outputCol='GenderEncoded')\nSI2 = StringIndexer(inputCol='Status',outputCol='StatusEncoded')\nSI3 = StringIndexer(inputCol='CarOwner',outputCol='CarOwnerEncoded')\nSI4 = StringIndexer(inputCol='Paymethod',outputCol='PaymethodEncoded')\nSI5 = StringIndexer(inputCol='LocalBilltype',outputCol='LocalBilltypeEncoded')\nSI6 = StringIndexer(inputCol='LongDistanceBilltype',outputCol='LongDistanceBilltypeEncoded')\n\n# Pipelines API requires that input variables are passed in  a vector\nassembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n                                       \"LongDistanceBilltypeEncoded\", \"Children\", \"EstIncome\", \"Age\", \"LongDistance\", \"International\", \"Local\",\\\n                                      \"Dropped\",\"Usage\",\"RatePlan\"], outputCol=\"features\")", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# encode the label column\nlabelIndexer = StringIndexer(inputCol='CHURN', outputCol='label').fit(data)", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# instantiate the algorithm, take the default settings\nrf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# build the pipeline\npipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6, labelIndexer, assembler, rf, labelConverter])", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# Split data into train and test datasets\n(trainingData, testingData) = data.randomSplit([0.7, 0.3],seed=9)\ntrainingData.cache()\ntestingData.cache()", "metadata": {}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# Build model. The fitted model from a Pipeline is a PipelineModel, which consists of fitted models and transformers, corresponding to the pipeline stages.\nmodel = pipeline.fit(trainingData)", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "### Step 7: Score the test data set"}, {"execution_count": null, "cell_type": "code", "source": "result=model.transform(testingData)\nresult_display=result.select(result[\"ID\"],result[\"CHURN\"],result[\"Label\"],result[\"predictedLabel\"],result[\"prediction\"],result[\"probability\"])\nresult_display.toPandas().head(6)", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 8: Model Evaluation\nFind accuracy of the models and the Area Under the ROC Curve "}, {"execution_count": null, "cell_type": "code", "source": "print 'Model Accuracy = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))", "metadata": {}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\nprint 'Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(result))", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "###  Step 9:  Tune the model to find the best model"}, {"metadata": {}, "cell_type": "markdown", "source": "#### Build a Parameter Grid specifying the parameters to be evaluated to determine the best combination"}, {"execution_count": null, "cell_type": "code", "source": "# set different levels for the maxDepth\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nparamGrid = (ParamGridBuilder().addGrid(rf.maxDepth,[4,6,8]).build())", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Create a cross validator to tune the pipeline with the generated parameter grid\nCross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."}, {"execution_count": null, "cell_type": "code", "source": "# perform 3 fold cross validation\ncv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# train the model\ncvModel = cv.fit(trainingData)\n\n# pick the best model\nbest_rfModel = cvModel.bestModel", "metadata": {"collapsed": true}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "# score the test data set with the best model\ncvresult=best_rfModel.transform(testingData)\ncvresults_show=cvresult.select(cvresult[\"ID\"],cvresult[\"CHURN\"],cvresult[\"Label\"],cvresult[\"predictedLabel\"],cvresult[\"prediction\"],cvresult[\"probability\"])\ncvresults_show.toPandas().head()", "metadata": {}, "outputs": []}, {"execution_count": null, "cell_type": "code", "source": "\nprint 'Model Accuracy of the best fitted model = {:.2f}.'.format(cvresult.filter(cvresult.label == cvresult.prediction).count()/ float(cvresult.count()))\nprint 'Model Accuracy of the default model = {:.2f}.'.format(result.filter(result.label == result.prediction).count() / float(result.count()))\nprint '   '\nprint('Area under the ROC curve of best fitted model = {:.2f}.'.format(evaluator.evaluate(cvresult)))\nprint 'Area under the ROC curve of the default model = {:.2f}.'.format(evaluator.evaluate(result))", "metadata": {}, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Step 10: Save Model\nSave the best model. \n\nA separate notebook has been created for \"batch scoring deployment\". This deployment notebook retrieves the saved model and applies it to a new dataset. The notebook can be scheduled to run via the Notebook scheduler or through the deployment interface in IBM WML."}, {"execution_count": null, "cell_type": "code", "source": "# Overwrite any existing saved model in the specified path\nbest_rfModel.write().overwrite().save(\"PredictChurn.churnModel\")", "metadata": {"collapsed": true}, "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "You have come to the end of this notebook"}, {"metadata": {}, "cell_type": "markdown", "source": "**Sidney Phoon**<br/>\n**Elena Lowery**<br/>\n**Rich Tarro**<br/>\n**Mokhtar Kandil**<br/>\nSeptember 1st, 2017"}], "nbformat": 4, "nbformat_minor": 1}